---
title: "Collecting metadata in large-scale projects"
subtitle: "A case study of the HCA integrated gut cell atlas"
date: "`r format(Sys.time(), '%d %B %Y')`"
author: "Kyle Kimler"
output: 
  html_document:
    self_contained: true
    css: style.css
---

```{r setup, include=FALSE}
library(reticulate)
use_virtualenv("/Users/kylekimler/pyenvs/metamanager", required = TRUE)
Sys.setenv(RETICULATE_PYTHON = "/Users/kylekimler/pyenvs/metamanager/bin/python")
```

# Introduction

Human experiments often have a complex design and a great deal of clinical covariates which can affect analysis. 
The collection of these covariates can itself be a difficult process, because they span multiple experimental levels. 
This is especially true in single-cell experiments, where thousands of cells are collected per sample, and sometimes multiple samples are collected per individual.
It is critical to analysis to organize these metadata sufficiently, and if we want to bring in prior data, we need to create or follow a standard metadata to be able to make comparisons.
Often, required metadata fields won't be included in publications or data repositories, so it is necessary to reach out to the authors of published data. 
When reaching out, it is important to provide a defined metadata sheet that can be easily read into analysis. For this, we use Google Sheets.

In this vignette, I will demonstrate methods for collecting and harmonizing metadata for human single-cell experiments on an atlas level.

This document provides a walkthrough of the metadata collection process used in the HCA integrated gut cell atlas project, 
including both the decision-making and technical steps involved.

## Workflow Overview

The first step in the metadata management process is to define the metadata. 

It is crucial to be as comprehensive as possible before we begin collecting data, 
but these methods are set up to make adding metadata fields later easier.

### Steps 1 and 2: Metadata Fields and Accepted Values

The HCA provides a required metadata schema that also includes metadata fields required by CELLxGENE. 
We adhere to this schema to make dataset upload to the portals easier. Any datasets that adhere can be uploaded to CELLxGENE and CAP, increasing accessibilty of these datasets to help them reach a wider audience.
The CELLxGENE-required and many HCA-recommended fields are termed "Tier 1 metadata", and do not include any patient-protected data. 
"Tier 2 metadata" includes all patient-protected data necessary for the analysis as well as project-specific metadata fields, in this case the fields specific to the gut atlas project.
These two metadata tiers should be collected separately. Tier 1 can be collected rapidly to make the datasets more accessible,
while Tier 2 metadata should be carefully considered by the project team and collected via secure channels.

Most metadata fields have some restrictions to what values can be entered. 
For example, fields like "manner_of_death" are coded to fit a standard, 
others like "development_stage_ontology_term_id" must adhere to a 10-year age range,
while others like "library_id" must follow a certain pattern - i.e. a string with no special characters.

First, we create a google sheet to define metadata fields and allowed values for each metadata field, which is shared and filled out by experimenters and those doing the analysis. 

![Metadata Workflow - Steps 1 and 2](/Users/kylekimler/Projects/GCA/graphics/GCA_metadata_workflow_1.png)

Next, this google sheet is used to format two metadata entry google sheets per dataset, tier 1 and tier 2, which can then be sent to authors or used for new experiments. 

I have written some Google Drive API functions to make communication with GDrive easier. These are provided here:
www.github.com/kylekimler/gdrive_api_extension.py

The HCA metadata schema is downloaded as a csv and used to display descriptions of each field at the top of the sheets.

First, set up and obtain the python code at: https://github.com/kylekimler/MetaManager-HCA

#### Python code to create Metadata Entry Google Sheets
```{python set up libraries, echo=TRUE}
from hca_metadata_manager.config import authenticate_with_google
from hca_metadata_manager.utils import initialize_google_sheets, add_metadata_descriptions
from hca_metadata_manager.workflow import generate_empty_metadata_entry_sheets, apply_dropdowns
import pandas as pd
# import scanpy as sc
```

```{python initialize google sheets, echo=TRUE}
gc = initialize_google_sheets()

credentials_file = "/Users/kylekimler/OAuth_kk_metadata_uploader2.json"

scopes = ['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']

credentials = authenticate_with_google(scopes, credentials_file)

# First, find the google sheet ID of the allowed metadata definitions sheet, found as suffix of the URL of the google sheet
# For example, if your google sheet is https://docs.google.com/spreadsheets/d/1eLBCEDKErmDK_nFIkOpATNbbBnnSVpoeqqPJ2uJKWrs
# Then the id is:
spreadsheet_id = '1eLBCEDKErmDK_nFIkOpATNbbBnnSVpoeqqPJ2uJKWrs'

# Next, open the google sheet using the GDrive API
spreadsheet = gc.open_by_key(spreadsheet_id)
```

Once you've opened the sheet with the API, you can convert it into a pandas dataframe.
```{python load definitions google sheet, echo = TRUE}
tabs = ['Tier 1 Dataset Metadata', 'Tier 1 Donor Metadata', 'Tier 1 Sample Metadata', 'Tier 2 Donor Metadata', 'Tier 2 Sample Metadata']
metadata_dfs = {}
for tab in tabs:
    worksheet = spreadsheet.worksheet(tab)
    data = worksheet.get_all_records()
    metadata_dfs[tab] = pd.DataFrame(data)

# And display what was in the sheet:
for tab, df in metadata_dfs.items():
    print(f"First few rows of {tab}:")
    print(df.head())
    break
```

Once you've downloaded your sheets containing the allowed metadata entries, we like to add examples and descriptions as our friends at HCA have created.
I have placed the descriptions sheet in the data/metadata_descriptions.csv. We can load and add these as header columns automatically using built-in functions here

```{python add metadata descriptions, echo=TRUE}
metadata_dfs = add_metadata_descriptions(metadata_dfs)
for tab, df in metadata_dfs.items():
    print(f"First few rows of {tab}:")
    print(df.head())
    break
```

Now that we have the outline for our entry google sheet, we need a google drive folder to store all our sheets. 
We can go to https://drive.google.com and create a new folder via the top left button. 
Then, navigate to the new folder and take the URL suffix which the API will need to find the folder.

For example, this folder: https://drive.google.com/drive/folders/1viymICEcfl5JOeVXxKxNDaQhTiHUEZKR,
the suffix is 1viymICEcfl5JOeVXxKxNDaQhTiHUEZKR.

And finally we can use the upload function to use the above Drive sheet to generate metadata entry sheets:

We can generate empty sheets to send to our collaborators as follows:
```{python generate metadata sheets, echo=TRUE}
folder_id = "1viymICEcfl5JOeVXxKxNDaQhTiHUEZKR"

generate_empty_metadata_entry_sheets(metadata_dfs, gc, credentials, folder_id, dataset_id = 'Kimler2025')

```

Or, we can prefill these sheets based on anndata objects we find online, using a different workflow function

```{python empty chunks, echo=TRUE}
# adata = sc.read_h5ad('/Users/kylekimler/gitHub/metaManager-HCA/data/pbmc3k_processed.h5ad')
# adata.obs['dataset_id'] = ['pbmc3k'] * adata.n_obs
# adata.obs['donor_id'] = np.random.choice(['Donor1', 'Donor2', 'Donor3'], size=adata.n_obs)
# adata.obs['sample_id'] = np.random.choice(['Sample1', 'Sample2'], size=adata.n_obs)
# adata.obs['author_cell_type'] = adata.obs['louvain']

# the folder_id is the suffix of the URL:
# upload_metadata_to_drive(adata, metadata_config, gc, credentials, folder_id)
```

### Steps 3 to 6: Metadata Handling

These metadata google sheets can be created ad hoc as above or for datasets that already exist.
When building an atlas, it can be a lot of work to pre-fill these based on data available online,
but it may help increase engagement from dataset authors when we request completion.

![Metadata Workflow - Steps 3 to 6](/Users/kylekimler/Projects/GCA/graphics/GCA_metadata_workflow_2.png)

### Code for generating and pre-filling a google sheet for an existing dataset

Once these google sheets are created, they can be easily downloaded if they are in a single folder.
Once we have them, we can create reports that help us determine how well the sheets were filled out.

#### Weekly Reporting

Some mistakes can be easily fixed, while others have to be addressed by authors. Since the google sheets are accessible online, anyone can edit them to help out!

### Atlas metrics

Lastly, these metadata can be used to create metrics about the project for presentation and analysis purposes.

# Conclusion

Summarize the key points covered in this tutorial and any conclusions or next steps.
